<html>
    <head>
        <title>InfoCon</title>
        <meta charset="UTF-8">
        <meta name="viewport" content="witdh=device-width, initial-scale=1.0">
        <style>
            body {
                font-family: "Arial";
            }
            table {
                width: 100%;
                border-collapse: collapse;
            }

            th, td {
                border: 5px solid rgb(255, 255, 255);
                background-color: rgb(240,240,240);
                padding: 8px;
                /* padding: 10px; */
                text-align: left;
                vertical-align: top;
            }

            .first-column {
                width: 600px;
            }
        </style>
    </head>

    <body>
    
    <!-- Title -->
    <table>
    <tr>
    <th colspan="2">
    <font size=8px> InfoCon: Concept Discovery with Generative and Discriminative Informativeness</font><br/>
    <font size=4px>&#x270D;&#xFE0F; Ruizhe Liu, Qian Luo, Yanchao Yang</font><br/>
    <font size=4px>&#x1F517; <a href="" target="_blank">[codes]</a><a href="https://openreview.net/pdf?id=g6eCbercEc" target="_blank">[paper]</a></font>
    </th>
    </tr>
    </table>
    <br/>
    <!-- Title End-->

    <!-- Introduction -->
    <table>
    <tr>
    <th colspan="2">
    <font size=6px>Introduction</font>
    </th>
    </tr>

    <tr>
    <td>
    <img src="imgs/infocon.png" width="600px"/>
    </td>
    <td>
    <font size=5px> <b>Discovery of Goals as Manipulation Concepts</b> </font><br/><br/>
    <font size=3px>
    Our research endeavors to discover manipulation concepts
    that characterize the goal an agent aim to fulfill at a specific juncture
    while interacting with the environment.<br/><br/>
    When provided with a dataset comprising demonstration trajectories of manipulation tasks,
    we are capable of segmenting these trajectories into distinct stages
    and assigning each stage with a unique label derived from a learnable codebook.
    This codebook serves as the symbolic representation of the goals,
    encapsulating the manipulation concepts we aim to uncover.
    </font>
    </td>
    </tr>

    <tr>
    <td class="first-column">
    <img src="imgs/gg_and_dg.jpg" width="600px"/>
    </td>
    <td>
    <font size=5px> <b>Model of Goal</b></font><br/><br/>
    <font size=3px>
    There are a myriad of methods to unearth latent knowledge from data.
    However, the question still remains: <b>How can we ascertain that it aligns with a 'goal'?</b>
    The essence of this inquiry is encapsulated in our understanding of <b>What constitutes a 'goal'?</b><br/><br/>
    Our research introduces two innovative concepts designed to ensure that the latent knowledge we uncover closely resembles a goal.<br/><br/>
    To encapsulate, the <b>Generative Goal</b> is indicative of the state that signifies the attainment of the goal.
    The <b>Discriminative Goal</b> assesses the appropriateness and completeness of a goal, thereby offering directional guidance.
    </font>
    </td>
    </tr>

    <tr>
    <td class="first-column">
    <img src="imgs/pipeline.png" width="600px"/>
    </td>
    <td>
    <font size=5px> <b>Self-Supervised Discovery of Manipulation Concepts</b></font><br/><br/>
    <font size=3px>
    Utilizing a VQVAE architecture,
    we have the capability to assign a specific label from the VQ codebook to
    every state within a demonstration trajectory.<br/><br/>
    Subsequently, leveraging the label proposed,
    we employ a self-supervised mechanism to
    refine the label assignments from the VQ codebook,
    thereby enhancing the segmentation of the trajectory.
    In this context, we utilize two predictive models to ensure that the
    latent manipulation concepts align with the definition of the Generative Goal and Discriminative Goal.
    </font>
    </td>
    </tr>

    </table>
    <br/>
    <!-- Introduction End-->

    <!-- Experiments -->
    <table>
    
    <tr>
    <th colspan="2">
    <font size=6px>Experiments on Manipulation Tasks</font>
    </th>
    </tr>

    <tr>
    <td class="first-column">
    <img src="imgs/cotpc.png" width="600px"/>
    From paper <a href="https://arxiv.org/abs/2304.00776" target="_blank">https://arxiv.org/abs/2304.00776</a>
    </td>
    <td>
    <font size=5px> <b>Evaluation based on CoTPC</b></font><br/><br/>
    <font size=3px>
    <a href="https://zjia.eng.ucsd.edu/cotpc" target="_blank">CoTPC</a> can help us evaluate the effectiveness of manipulation concepts discovered by InfoCon.<br/><br/>
    CoTPC can improve performance of behavior cloning on manipulation tasks
    by learning to predict control and action along with what called key states.
    Here key states can be regarded as achievement of a certain sub-goal.<br/><br/>
    Original work of CoTPC relies on the semantic label of key states, aligning with human intuition.
    We can just change the key states to the ones discovered by InfoCon,
    to see whether the concepts of InfoCon is more helpful.
    </font>
    </td>
    </tr>

    <tr>
    <td class="first-column">
    <img src="imgs/main_results.png" width="600px"/>
    </td>
    <td>
    <font size=5px> <b>Comparison between Baselines</b></font><br/><br/>
    <font size=3px>
    We carry out our experiments on four tasks from Maniskill2.
    Here every task has key states aligning with semantic intuition provided by CoTPC research.<br/><br/>
    For methods which do not use key states,
    we only give the results of decision transformer,for it performs the best.
    Other baselines here use different methods to label out key states,
    which do not necessarily have to do with something similar to semantic concepts.
    We can see that the performance of our method is superior comparing to other key states discovery methods.

    
    </font>
    </td>
    </tr>
    
    </table>
    <!-- Experiments End-->

    </body>
</html>
